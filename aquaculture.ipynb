{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRFJA+ONsF0HEkR3uZ+PbL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manii5228/Aquaculture/blob/main/aquaculture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzFHNyakOTmT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "6qjcg_xHOfyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                             accuracy_score, precision_score, recall_score) # Added classification metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "import os # Import os to check for file existence\n",
        "\n",
        "# === Step 1: Load and Preprocess Data ===\n",
        "def load_and_preprocess(filepath, downsample_factor=4):\n",
        "    \"\"\"\n",
        "    Loads the salinity dataset, preprocesses it by downsampling, flipping,\n",
        "    imputing missing values, and standardizing.\n",
        "    Prepares X (input images) and y (target mean salinity for the next step).\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the NetCDF file containing salinity data.\n",
        "        downsample_factor (int): Factor by which to downsample the spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y, scaler) where X is the preprocessed input,\n",
        "               y is the target output, and scaler is the StandardScaler object.\n",
        "    \"\"\"\n",
        "    # Check if the file exists before attempting to open\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Dataset file not found at '{filepath}'\")\n",
        "        print(\"Please ensure the 'salinity.nc' file is in the specified path.\")\n",
        "        # Exit or raise an error as the program cannot proceed without the file\n",
        "        raise FileNotFoundError(f\"Dataset file not found at {filepath}\")\n",
        "\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    ds = xr.open_dataset(filepath, decode_times=False)\n",
        "    # Select 'SALT' variable, take the first depth layer, and convert to numpy array\n",
        "    salt = ds['SALT'].values[:, 0, :, :]  # Remove depth dimension\n",
        "\n",
        "    print(f\"Original data shape: {salt.shape}\")\n",
        "\n",
        "    # Downsample and flip vertically to correct orientation\n",
        "    # Downsampling reduces spatial dimensions by the downsample_factor\n",
        "    salt = salt[:, ::downsample_factor, ::downsample_factor]\n",
        "    # Flipping along axis=1 (latitude) to correct map orientation\n",
        "    salt = np.flip(salt, axis=1)\n",
        "\n",
        "    print(f\"Downsampled and flipped data shape: {salt.shape}\")\n",
        "\n",
        "    # Replace NaNs with interpolation (mean imputation per time slice)\n",
        "    # Iterate through each time slice and apply SimpleImputer\n",
        "    print(\"Imputing missing values...\")\n",
        "    for i in range(len(salt)):\n",
        "        # Reshape to 2D for SimpleImputer, then reshape back\n",
        "        imputed_slice = SimpleImputer(strategy='mean').fit_transform(salt[i].reshape(-1, 1)).reshape(salt[i].shape)\n",
        "        salt[i] = imputed_slice\n",
        "    print(\"Missing values imputed.\")\n",
        "\n",
        "    # Standardize globally (not per-slice) for consistent scaling across the entire dataset\n",
        "    print(\"Standardizing data...\")\n",
        "    scaler = StandardScaler()\n",
        "    # Reshape the 3D array (time, lat, lon) into 2D (total_elements, 1) for fitting the scaler\n",
        "    salt_reshaped = salt.reshape(-1, salt.shape[-1] * salt.shape[-2])\n",
        "    # Fit and transform the data, then reshape back to original 3D shape\n",
        "    salt_scaled = scaler.fit_transform(salt_reshaped).reshape(salt.shape)\n",
        "    print(\"Data standardization complete.\")\n",
        "\n",
        "    # Prepare X (input features) and y (target labels)\n",
        "    # X will be the salinity map at time t\n",
        "    X = salt_scaled[:-1]\n",
        "    # y will be the mean salinity of the map at time t+1\n",
        "    y = np.mean(salt_scaled[1:], axis=(1, 2))\n",
        "\n",
        "    # Add channel dimension for CNN: (batch_size, height, width, channels)\n",
        "    # For grayscale images, channels = 1\n",
        "    X = X[..., np.newaxis]\n",
        "    print(f\"Final X shape for CNN: {X.shape}\")\n",
        "    print(f\"Final y shape: {y.shape}\")\n",
        "    return X, y, scaler\n",
        "\n",
        "# Define the path to your dataset.\n",
        "# IMPORTANT: Ensure 'salinity.nc' is present at this path in your environment.\n",
        "# If running in Colab, this means it needs to be in your mounted Google Drive.\n",
        "DATASET_FILEPATH = \"/content/drive/MyDrive/salinity.nc\"\n",
        "\n",
        "# === Step 2: Downsample and Split Data ===\n",
        "try:\n",
        "    X, y, scaler = load_and_preprocess(\n",
        "        DATASET_FILEPATH,\n",
        "        downsample_factor=4  # Original 410x720 â†’ Downsampled to 102x180\n",
        "    )\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    print(\"Exiting program. Please provide the correct dataset path.\")\n",
        "    exit() # Exit if the file is not found\n",
        "\n",
        "# Split data into Train/Validation/Test sets\n",
        "# Training: 60%, Validation: 20%, Test: 20%\n",
        "print(\"\\nSplitting data into training, validation, and test sets (60/20/20)...\")\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# === Step 3: Build CNN Model ===\n",
        "def build_model(input_shape):\n",
        "    \"\"\"\n",
        "    Builds a Convolutional Neural Network (CNN) model for regression.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input data (height, width, channels).\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled CNN model.\n",
        "    \"\"\"\n",
        "    print(\"\\nBuilding CNN model...\")\n",
        "    model = Sequential([\n",
        "        # First Convolutional Block\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01), padding='same'),\n",
        "        BatchNormalization(), # Normalizes activations of previous layer\n",
        "        MaxPooling2D((2, 2)), # Reduces spatial dimensions, helps in extracting dominant features\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Flatten(), # Flattens the 2D feature maps into a 1D vector for Dense layers\n",
        "\n",
        "        # Dense (Fully Connected) Layers\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2), # Randomly sets a fraction of input units to 0 at each update during training, prevents overfitting\n",
        "        Dense(1) # Output layer for regression, single unit with linear activation\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    # Optimizer: Adam is a popular choice for its efficiency\n",
        "    # Loss: Mean Squared Error (MSE) is common for regression tasks\n",
        "    # Metrics: Mean Absolute Error (MAE) provides another view on error magnitude\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    print(\"CNN model built and compiled.\")\n",
        "    model.summary() # Print model summary to see layer details\n",
        "    return model\n",
        "\n",
        "# Build the model using the input shape derived from the training data\n",
        "model = build_model(X_train.shape[1:])\n",
        "\n",
        "# === Step 4: Train with Early Stopping ===\n",
        "print(\"\\nStarting model training...\")\n",
        "# EarlyStopping callback monitors a validation metric and stops training\n",
        "# when the metric has stopped improving for a specified number of epochs (patience).\n",
        "# restore_best_weights ensures that the model weights are set to the epoch with the best monitored value.\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50, # Maximum number of epochs\n",
        "    batch_size=64, # Number of samples per gradient update\n",
        "    validation_data=(X_val, y_val), # Data on which to evaluate the loss and any model metrics at the end of each epoch\n",
        "    callbacks=[early_stopping], # List of callbacks to apply during training\n",
        "    verbose=1 # Show training progress bar\n",
        ")\n",
        "print(\"Model training finished.\")\n",
        "\n",
        "\n",
        "# === Step 5: Evaluate on Test Set ===\n",
        "print(\"\\nEvaluating model on test set...\")\n",
        "# Predict on the test set\n",
        "y_pred_raw = model.predict(X_test)\n",
        "y_pred = y_pred_raw.flatten() # Flatten predictions to match y_test shape\n",
        "\n",
        "# --- Regression Metrics ---\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred) # Calculate MAE explicitly\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nðŸ“Š Final Regression Test Metrics:\")\n",
        "print(f\"âœ… MSE (Mean Squared Error): {mse:.6f}\")\n",
        "print(f\"âœ… RMSE (Root Mean Squared Error): {rmse:.6f}\")\n",
        "print(f\"âœ… MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "print(f\"âœ… RÂ² (R-squared): {r2:.4f}\")\n",
        "\n",
        "# --- Classification Metrics (for Demonstration Purposes Only) ---\n",
        "print(\"\\nðŸ”„ Classification Metrics (Derived for Demonstration - See Note Below):\")\n",
        "\n",
        "# Define a threshold to convert regression output to binary classification.\n",
        "# Using the median of the true test values as a simple example threshold.\n",
        "# Values > threshold are class 1 (e.g., \"high salinity\"), else class 0 (\"not high salinity\").\n",
        "classification_threshold = np.median(y_test)\n",
        "print(f\"  (Using classification threshold: {classification_threshold:.4f} based on median of true test values)\")\n",
        "\n",
        "# Convert true and predicted continuous values to binary classes based on the threshold\n",
        "y_test_binary = (y_test > classification_threshold).astype(int)\n",
        "y_pred_binary = (y_pred > classification_threshold).astype(int)\n",
        "\n",
        "# Calculate classification metrics\n",
        "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "precision = precision_score(y_test_binary, y_pred_binary, zero_division=0) # zero_division=0 handles cases with no positive predictions\n",
        "recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "\n",
        "print(f\"âœ… Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"âœ… Precision: {precision:.4f}\")\n",
        "print(f\"âœ… Recall:    {recall:.4f}\")\n",
        "\n",
        "print(\"\\n--- Important Note on Classification Metrics ---\")\n",
        "print(\"The 'Accuracy', 'Precision', and 'Recall' metrics above are calculated by converting the continuous regression \"\n",
        "      \"outputs into binary classes using an arbitrary threshold (the median of true test values in this case).\")\n",
        "print(\"This transformation is done *only for demonstration* of these metrics. Your model's primary task is regression, \"\n",
        "      \"and its performance should be primarily judged by MSE, RMSE, MAE, and R-squared.\")\n",
        "print(\"Choosing a different threshold would likely change these classification metric values.\")\n",
        "\n",
        "# === Step 6: Visualize Results ===\n",
        "print(\"\\nGenerating plots for visualization...\")\n",
        "plt.figure(figsize=(14, 6)) # Increased figure size for better readability\n",
        "\n",
        "# Plot True vs Predicted values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(y_test, label='True Mean SALT', marker='o', linestyle='-', color='blue', markersize=4)\n",
        "plt.plot(y_pred, label='Predicted Mean SALT', marker='x', linestyle='--', color='red', markersize=4)\n",
        "plt.title(\"True vs. Predicted Mean Salinity (Standardized)\")\n",
        "plt.xlabel(\"Time Step Index\")\n",
        "plt.ylabel(\"Mean Salinity (Standardized)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
        "\n",
        "# Plot a sample input image\n",
        "plt.subplot(1, 2, 2)\n",
        "# Display the first image from the test set, removing the channel dimension for imshow\n",
        "# 'origin='lower'' ensures that the image is displayed with (0,0) at the bottom-left,\n",
        "# which is common for scientific data plots.\n",
        "plt.imshow(X_test[0, :, :, 0], cmap='viridis', origin='lower')\n",
        "plt.title(\"Sample Input Salinity Map (Corrected Orientation)\")\n",
        "plt.colorbar(label=\"Standardized Salinity Value\")\n",
        "plt.xlabel(\"Longitude Index\")\n",
        "plt.ylabel(\"Latitude Index\")\n",
        "plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nScript execution complete. Please check the plots for visualization.\")"
      ],
      "metadata": {
        "id": "D1ulwlbaOZYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-LSTM"
      ],
      "metadata": {
        "id": "2PUFwpqiOivl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                             accuracy_score, precision_score, recall_score) # Added classification metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten,\n",
        "                                     Dense, Dropout, BatchNormalization,\n",
        "                                     LSTM, TimeDistributed)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# === Step 1: Enhanced Data Loading and Preprocessing ===\n",
        "def load_and_preprocess(filepath, downsample_factor=4, seq_length=3):\n",
        "    \"\"\"\n",
        "    Loads the salinity dataset, preprocesses it, creates temporal sequences,\n",
        "    and standardizes the data.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the NetCDF file containing salinity data.\n",
        "        downsample_factor (int): Factor by which to downsample the spatial dimensions.\n",
        "        seq_length (int): Number of time steps to use as input sequence for prediction.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y, scaler) where X is the preprocessed input sequences,\n",
        "               y is the target mean salinity, and scaler is the StandardScaler object.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Dataset file not found at '{filepath}'\")\n",
        "        print(\"Please ensure the 'salinity.nc' file is in the specified path.\")\n",
        "        raise FileNotFoundError(f\"Dataset file not found at {filepath}\")\n",
        "\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    ds = xr.open_dataset(filepath, decode_times=False)\n",
        "    salt = ds['SALT'].values[:, 0, :, :]\n",
        "\n",
        "    print(f\"Original data shape: {salt.shape}\")\n",
        "\n",
        "    salt = salt[:, ::downsample_factor, ::downsample_factor]\n",
        "    salt = np.flip(salt, axis=1)\n",
        "\n",
        "    print(f\"Downsampled and flipped data shape: {salt.shape}\")\n",
        "\n",
        "    print(\"Imputing missing values using median strategy...\")\n",
        "    salt = np.array([SimpleImputer(strategy='median').fit_transform(s.reshape(-1, 1)).reshape(s.shape)\n",
        "                     for s in salt])\n",
        "    print(\"Missing values imputed.\")\n",
        "\n",
        "    print(\"Standardizing data globally...\")\n",
        "    scaler = StandardScaler()\n",
        "    salt_flat = salt.reshape(-1, salt.shape[1] * salt.shape[2])\n",
        "    salt_scaled = scaler.fit_transform(salt_flat).reshape(salt.shape)\n",
        "    print(\"Data standardization complete.\")\n",
        "\n",
        "    print(f\"Creating temporal sequences with length: {seq_length}...\")\n",
        "    X_seq, y_target = [], []\n",
        "    for i in range(len(salt_scaled) - seq_length):\n",
        "        X_seq.append(salt_scaled[i:i+seq_length])\n",
        "        y_target.append(np.mean(salt_scaled[i+seq_length]))\n",
        "\n",
        "    X_seq = np.array(X_seq)\n",
        "    y_target = np.array(y_target)\n",
        "\n",
        "    X_seq = X_seq[..., np.newaxis]\n",
        "\n",
        "    print(f\"Final X (input sequences) shape: {X_seq.shape}\")\n",
        "    print(f\"Final y (target mean salinity) shape: {y_target.shape}\")\n",
        "\n",
        "    if np.var(y_target) < 0.01:\n",
        "        print(\"Warning: Target variance is very low. This might indicate that predicting the mean salinity \"\n",
        "              \"is too simple or there's not enough variability to learn. Consider predicting \"\n",
        "              \"spatial patterns or specific points instead.\")\n",
        "\n",
        "    return X_seq, y_target, scaler\n",
        "\n",
        "# Define the path to your dataset.\n",
        "DATASET_FILEPATH = \"/content/drive/MyDrive/salinity.nc\"\n",
        "SEQUENCE_LENGTH = 3\n",
        "\n",
        "# === Step 2: Data Preparation ===\n",
        "try:\n",
        "    X, y, scaler = load_and_preprocess(\n",
        "        DATASET_FILEPATH,\n",
        "        downsample_factor=4,\n",
        "        seq_length=SEQUENCE_LENGTH\n",
        "    )\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    print(\"Exiting program. Please provide the correct dataset path.\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nSplitting data into training, validation, and test sets with temporal ordering (60/20/20)...\")\n",
        "train_size = int(0.6 * len(X))\n",
        "val_size = int(0.2 * len(X))\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# === Step 3: Hybrid CNN-LSTM Model ===\n",
        "def build_model(input_shape):\n",
        "    \"\"\"\n",
        "    Builds a Hybrid CNN-LSTM model for sequence prediction.\n",
        "    \"\"\"\n",
        "    print(\"\\nBuilding Hybrid CNN-LSTM model...\")\n",
        "    model = Sequential([\n",
        "        TimeDistributed(Conv2D(32, (3,3), activation='relu', kernel_regularizer=l2(0.001), padding='same'), input_shape=input_shape),\n",
        "        TimeDistributed(BatchNormalization()),\n",
        "        TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "        TimeDistributed(Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.001), padding='same')),\n",
        "        TimeDistributed(BatchNormalization()),\n",
        "        TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "        # Added another Conv2D layer for more feature extraction capacity\n",
        "        TimeDistributed(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(0.001), padding='same')),\n",
        "        TimeDistributed(BatchNormalization()),\n",
        "        TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "        TimeDistributed(Flatten()),\n",
        "\n",
        "        # Increased LSTM units for more capacity\n",
        "        LSTM(256, return_sequences=False, kernel_regularizer=l2(0.001)), # Changed from 128 to 256\n",
        "\n",
        "        # Increased Dense units for more capacity\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)), # Changed from 64 to 128\n",
        "        # Adjusted Dropout rate for stronger regularization\n",
        "        Dropout(0.5), # Changed from 0.4\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        # Adjusted Adam learning rate\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Changed from 0.0005\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    print(\"Hybrid CNN-LSTM model built and compiled.\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = build_model(X_train.shape[1:])\n",
        "\n",
        "# === Step 4: Training with Callbacks ===\n",
        "print(\"\\nStarting model training...\")\n",
        "callbacks = [\n",
        "    # Increased patience for EarlyStopping\n",
        "    tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'), # Changed from 10\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, monitor='val_loss', min_lr=0.00001)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    # Adjusted batch size\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Model training finished.\")\n",
        "\n",
        "# === Step 5: Evaluation ===\n",
        "print(\"\\nEvaluating model on test set...\")\n",
        "y_pred_raw = model.predict(X_test)\n",
        "y_pred = y_pred_raw.flatten()\n",
        "\n",
        "# --- Regression Metrics ---\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nðŸ“Š Regression Test Metrics:\")\n",
        "print(f\"âœ… MSE (Mean Squared Error): {mse:.6f}\")\n",
        "print(f\"âœ… RMSE (Root Mean Squared Error): {rmse:.6f}\")\n",
        "print(f\"âœ… MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "print(f\"âœ… RÂ² (R-squared): {r2:.4f}\")\n",
        "\n",
        "# --- Classification Metrics (for Demonstration Purposes Only) ---\n",
        "print(\"\\nðŸ”„ Classification Metrics (Derived for Demonstration - See Note Below):\")\n",
        "\n",
        "classification_threshold = np.median(y_test)\n",
        "print(f\"  (Using classification threshold: {classification_threshold:.4f} based on median of true test values)\")\n",
        "\n",
        "y_test_binary = (y_test > classification_threshold).astype(int)\n",
        "y_pred_binary = (y_pred > classification_threshold).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "precision = precision_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "\n",
        "print(f\"âœ… Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"âœ… Precision: {precision:.4f}\")\n",
        "print(f\"âœ… Recall:    {recall:.4f}\")\n",
        "\n",
        "print(\"\\n--- Important Note on Classification Metrics ---\")\n",
        "print(\"The 'Accuracy', 'Precision', and 'Recall' metrics above are calculated by converting the continuous regression \"\n",
        "      \"outputs into binary classes using an arbitrary threshold (the median of true test values in this case).\")\n",
        "print(\"This transformation is done *only for demonstration* of these metrics. Your model's primary task is regression, \"\n",
        "      \"and its performance should be primarily judged by MSE, RMSE, MAE, and R-squared.\")\n",
        "print(\"Choosing a different threshold would likely change these classification metric values.\")\n",
        "\n",
        "\n",
        "# === Step 6: Enhanced Visualization ===\n",
        "print(\"\\nGenerating plots for visualization...\")\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Subplot 1: True vs. Predicted Values Scatter Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, y_pred, alpha=0.6, color='blue', edgecolors='w', s=50)\n",
        "plt.plot([min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())],\n",
        "         [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())],\n",
        "         'r--', label='Perfect Prediction')\n",
        "plt.xlabel(\"True Values (Standardized Mean Salinity)\", fontsize=12)\n",
        "plt.ylabel(\"Predictions (Standardized Mean Salinity)\", fontsize=12)\n",
        "plt.title(\"Prediction Accuracy: True vs. Predicted Values\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.axis('equal')\n",
        "\n",
        "# --- Plot True vs. Predicted Mean Salinity (Standardized) ---\n",
        "plt.figure(figsize=(10, 6)) # Adjust figure size as needed for a single plot\n",
        "\n",
        "plt.plot(y_test, label='True Mean SALT', marker='o', linestyle='-', color='blue', markersize=4)\n",
        "plt.plot(y_pred, label='Predicted Mean SALT', marker='x', linestyle='--', color='red', markersize=4)\n",
        "plt.title(\"True vs. Predicted Mean Salinity (Standardized)\")\n",
        "plt.xlabel(\"Time Step Index\")\n",
        "plt.ylabel(\"Mean Salinity (Standardized)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
        "\n",
        "plt.show()\n",
        "print(\"\\nScript execution complete. Please check the console output for metrics and the plots for visualization.\")\n"
      ],
      "metadata": {
        "id": "Xqa36h6aOgvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-UNET"
      ],
      "metadata": {
        "id": "kHHglfaAOxqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                             accuracy_score, precision_score, recall_score) # Added classification metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, UpSampling2D,\n",
        "                                     Concatenate, Input, BatchNormalization, Cropping2D) # Import Cropping2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# === 1. Data Loading with Shape Control ===\n",
        "def load_data(filepath, target_shape=(100, 180)):\n",
        "    \"\"\"\n",
        "    Loads the salinity dataset, preprocesses it by downsampling, correcting orientation,\n",
        "    imputing NaNs, and standardizing.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the NetCDF file containing salinity data.\n",
        "        target_shape (tuple): Desired (height, width) for the spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (salt_data, scaler) where salt_data is the preprocessed and shaped\n",
        "               salinity data, and scaler is the StandardScaler object.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Dataset file not found at '{filepath}'\")\n",
        "        print(\"Please ensure the 'salinity.nc' file is in the specified path.\")\n",
        "        return None, None # Return None to indicate failure\n",
        "\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    ds = xr.open_dataset(filepath, decode_times=False)\n",
        "    salt = ds['SALT'].values[:, 0, :, :]  # Select 'SALT', remove depth dim\n",
        "\n",
        "    print(f\"Original data shape: {salt.shape}\")\n",
        "\n",
        "    # Calculate current dimensions\n",
        "    h_orig, w_orig = salt.shape[1], salt.shape[2]\n",
        "\n",
        "    # Calculate downsample factors to reach target_shape\n",
        "    # This approach handles cases where original dimensions are not direct multiples\n",
        "    downsample_factor_h = h_orig // target_shape[0]\n",
        "    downsample_factor_w = w_orig // target_shape[1]\n",
        "\n",
        "    # Ensure factors are at least 1 to avoid upsampling or division by zero\n",
        "    downsample_factor_h = max(1, downsample_factor_h)\n",
        "    downsample_factor_w = max(1, downsample_factor_w)\n",
        "\n",
        "    # Downsample using calculated factors\n",
        "    salt = salt[:, ::downsample_factor_h, ::downsample_factor_w]\n",
        "\n",
        "    # Crop to exactly target_shape if dimensions are slightly larger after downsampling\n",
        "    # This might happen if original dimensions are not perfect multiples\n",
        "    if salt.shape[1] > target_shape[0]:\n",
        "        salt = salt[:, :target_shape[0], :]\n",
        "    if salt.shape[2] > target_shape[1]:\n",
        "        salt = salt[:, :, :target_shape[1]]\n",
        "\n",
        "\n",
        "    salt = np.flip(salt, axis=1)  # Correct orientation (e.g., flip latitude)\n",
        "\n",
        "    print(f\"Downsampled and flipped data shape: {salt.shape}\")\n",
        "\n",
        "    print(\"Imputing missing values using median strategy...\")\n",
        "    # Apply median imputation slice by slice\n",
        "    salt_imputed = []\n",
        "    for s_slice in salt:\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        # Reshape for imputer (2D array, column-wise imputation) and then reshape back\n",
        "        imputed_slice = imputer.fit_transform(s_slice.reshape(-1, 1)).reshape(s_slice.shape)\n",
        "        salt_imputed.append(imputed_slice)\n",
        "    salt = np.array(salt_imputed)\n",
        "    print(\"Missing values imputed.\")\n",
        "\n",
        "    print(\"Standardizing data globally...\")\n",
        "    scaler = StandardScaler()\n",
        "    # Reshape the 3D array (time, height, width) to 2D (total_elements, 1) for scaler\n",
        "    salt_reshaped_for_scaler = salt.reshape(-1, target_shape[0] * target_shape[1])\n",
        "    salt_scaled = scaler.fit_transform(salt_reshaped_for_scaler)\n",
        "    # Reshape back to original 3D structure for spatial data\n",
        "    salt_data = salt_scaled.reshape(-1, *target_shape)\n",
        "\n",
        "    # Add channel dimension for CNN input (batch, height, width, channels)\n",
        "    salt_data = salt_data[..., np.newaxis]\n",
        "    print(f\"Final preprocessed data shape: {salt_data.shape}\")\n",
        "    print(\"Data standardization complete.\")\n",
        "\n",
        "    return salt_data, scaler\n",
        "\n",
        "# === 2. U-Net with Shape Matching ===\n",
        "def build_unet(input_shape):\n",
        "    \"\"\"\n",
        "    Builds a U-Net model for image-to-image regression.\n",
        "    The U-Net architecture is designed to capture features at multiple scales\n",
        "    and produce a segmentation-like output (a predicted map in this case).\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input images (height, width, channels).\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled U-Net model.\n",
        "    \"\"\"\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder (Downsampling Path)\n",
        "    # Block 1\n",
        "    c1 = Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    c1 = Conv2D(32, (3,3), activation='relu', padding='same')(c1)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    p1 = MaxPooling2D((2,2))(c1)\n",
        "    print(f\"Shape after Block 1: {p1.shape}\")\n",
        "\n",
        "    # Block 2\n",
        "    c2 = Conv2D(64, (3,3), activation='relu', padding='same')(p1)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    c2 = Conv2D(64, (3,3), activation='relu', padding='same')(c2)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    p2 = MaxPooling2D((2,2))(c2)\n",
        "    print(f\"Shape after Block 2: {p2.shape}\")\n",
        "\n",
        "\n",
        "    # Block 3\n",
        "    c3 = Conv2D(128, (3,3), activation='relu', padding='same')(p2)\n",
        "    c3 = BatchNormalization()(c3)\n",
        "    c3 = Conv2D(128, (3,3), activation='relu', padding='same')(c3)\n",
        "    c3 = BatchNormalization()(c3)\n",
        "    p3 = MaxPooling2D((2,2))(c3)\n",
        "    print(f\"Shape after Block 3: {p3.shape}\")\n",
        "\n",
        "\n",
        "    # Bridge (Bottleneck)\n",
        "    c4 = Conv2D(256, (3,3), activation='relu', padding='same')(p3)\n",
        "    c4 = BatchNormalization()(c4)\n",
        "    c4 = Conv2D(256, (3,3), activation='relu', padding='same')(c4)\n",
        "    c4 = BatchNormalization()(c4)\n",
        "    print(f\"Shape after Bridge: {c4.shape}\")\n",
        "\n",
        "    # Decoder (Upsampling Path)\n",
        "    # Block 5\n",
        "    u5 = UpSampling2D((2,2))(c4)\n",
        "    print(f\"Shape after UpSampling 1: {u5.shape}\")\n",
        "\n",
        "    # Crop c3 to match the spatial dimensions of u5 before concatenation\n",
        "    shape_diff_h_c3_u5 = c3.shape[1] - u5.shape[1]\n",
        "    shape_diff_w_c3_u5 = c3.shape[2] - u5.shape[2]\n",
        "    crop_top_c3_u5 = shape_diff_h_c3_u5 // 2\n",
        "    crop_bottom_c3_u5 = shape_diff_h_c3_u5 - crop_top_c3_u5\n",
        "    crop_left_c3_u5 = shape_diff_w_c3_u5 // 2\n",
        "    crop_right_c3_u5 = shape_diff_w_c3_u5 - crop_left_c3_u5\n",
        "    c3_cropped = Cropping2D(cropping=((crop_top_c3_u5, crop_bottom_c3_u5), (crop_left_c3_u5, crop_right_c3_u5)))(c3)\n",
        "    print(f\"Shape of c3_cropped for concat 1: {c3_cropped.shape}\")\n",
        "    u5 = Concatenate()([u5, c3_cropped])\n",
        "    print(f\"Shape after Concat 1: {u5.shape}\")\n",
        "    c5 = Conv2D(128, (3,3), activation='relu', padding='same')(u5)\n",
        "    c5 = BatchNormalization()(c5)\n",
        "    c5 = Conv2D(128, (3,3), activation='relu', padding='same')(c5)\n",
        "    c5 = BatchNormalization()(c5)\n",
        "    print(f\"Shape after Conv Block 5: {c5.shape}\")\n",
        "\n",
        "\n",
        "    # Block 6\n",
        "    u6 = UpSampling2D((2,2))(c5)\n",
        "    print(f\"Shape after UpSampling 2: {u6.shape}\")\n",
        "\n",
        "    # Crop c2 to match the spatial dimensions of u6 before concatenation\n",
        "    shape_diff_h_c2_u6 = c2.shape[1] - u6.shape[1]\n",
        "    shape_diff_w_c2_u6 = c2.shape[2] - u6.shape[2]\n",
        "    crop_top_c2_u6 = shape_diff_h_c2_u6 // 2\n",
        "    crop_bottom_c2_u6 = shape_diff_h_c2_u6 - crop_top_c2_u6\n",
        "    crop_left_c2_u6 = shape_diff_w_c2_u6 // 2\n",
        "    crop_right_c2_u6 = shape_diff_w_c2_u6 - crop_left_c2_u6\n",
        "    c2_cropped = Cropping2D(cropping=((crop_top_c2_u6, crop_bottom_c2_u6), (crop_left_c2_u6, crop_right_c2_u6)))(c2)\n",
        "    print(f\"Shape of c2_cropped for concat 2: {c2_cropped.shape}\")\n",
        "    u6 = Concatenate()([u6, c2_cropped])\n",
        "    print(f\"Shape after Concat 2: {u6.shape}\")\n",
        "    c6 = Conv2D(64, (3,3), activation='relu', padding='same')(u6)\n",
        "    c6 = BatchNormalization()(c6)\n",
        "    c6 = Conv2D(64, (3,3), activation='relu', padding='same')(c6)\n",
        "    c6 = BatchNormalization()(c6)\n",
        "    print(f\"Shape after Conv Block 6: {c6.shape}\")\n",
        "\n",
        "\n",
        "    # Block 7\n",
        "    u7 = UpSampling2D((2,2))(c6)\n",
        "    print(f\"Shape after UpSampling 3: {u7.shape}\")\n",
        "\n",
        "    # Crop c1 to match the spatial dimensions of u7 before concatenation\n",
        "    shape_diff_h_c1_u7 = c1.shape[1] - u7.shape[1]\n",
        "    shape_diff_w_c1_u7 = c1.shape[2] - u7.shape[2]\n",
        "    crop_top_c1_u7 = shape_diff_h_c1_u7 // 2\n",
        "    crop_bottom_c1_u7 = shape_diff_h_c1_u7 - crop_top_c1_u7\n",
        "    crop_left_c1_u7 = shape_diff_w_c1_u7 // 2\n",
        "    crop_right_c1_u7 = shape_diff_w_c1_u7 - crop_left_c1_u7\n",
        "    c1_cropped = Cropping2D(cropping=((crop_top_c1_u7, crop_bottom_c1_u7), (crop_left_c1_u7, crop_right_c1_u7)))(c1)\n",
        "    print(f\"Shape of c1_cropped for concat 3: {c1_cropped.shape}\")\n",
        "    u7 = Concatenate()([u7, c1_cropped])\n",
        "    print(f\"Shape after Concat 3: {u7.shape}\")\n",
        "    c7 = Conv2D(32, (3,3), activation='relu', padding='same')(u7)\n",
        "    c7 = BatchNormalization()(c7)\n",
        "    c7 = Conv2D(32, (3,3), activation='relu', padding='same')(c7)\n",
        "    c7 = BatchNormalization()(c7)\n",
        "    print(f\"Shape after Conv Block 7: {c7.shape}\")\n",
        "\n",
        "\n",
        "    # Output layer: 1x1 convolution to produce a single channel output (for salinity map)\n",
        "    outputs = Conv2D(1, (1,1), activation='linear')(c7)\n",
        "    print(f\"Shape after Output Layer: {outputs.shape}\")\n",
        "\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
        "    print(\"U-Net model built and compiled.\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Define the path to your dataset.\n",
        "DATASET_FILEPATH = \"/content/drive/MyDrive/salinity.nc\"\n",
        "TARGET_IMAGE_SHAPE = (96,176) # Desired height x width for images\n",
        "SEQUENCE_LENGTH = 3 # Number of past frames to consider for predicting the next frame\n",
        "\n",
        "# === 3. Data Preparation ===\n",
        "salt_data, scaler = load_data(DATASET_FILEPATH, target_shape=TARGET_IMAGE_SHAPE)\n",
        "if salt_data is None:\n",
        "    raise ValueError(\"Failed to load data. Please check file path and contents.\")\n",
        "\n",
        "# Create sequences: X is the last frame of a sequence, y is the frame to predict\n",
        "X, y = [], []\n",
        "for i in range(len(salt_data) - SEQUENCE_LENGTH):\n",
        "    # Input X: The last frame of the sequence (t, t+1, t+2 -> use t+2 to predict t+3)\n",
        "    X.append(salt_data[i + SEQUENCE_LENGTH - 1])\n",
        "    # Target y: The frame immediately following the sequence (t+3)\n",
        "    y.append(salt_data[i + SEQUENCE_LENGTH])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"X (input frames) shape: {X.shape}\") # Should be (n_samples, H, W, C)\n",
        "print(f\"y (target frames) shape: {y.shape}\") # Should be (n_samples, H, W, C)\n",
        "\n",
        "# Time-series cross-validation\n",
        "# We use TimeSeriesSplit to ensure no data leakage from future time steps\n",
        "tscv = TimeSeriesSplit(n_splits=2) # Using 2 splits for demonstration; adjust for more robust evaluation\n",
        "\n",
        "# Initialize lists to store metrics from each fold\n",
        "all_mse, all_rmse, all_mae, all_r2 = [], [], [], []\n",
        "all_accuracy, all_precision, all_recall = [], [], []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
        "    print(f\"\\n=== Fold {fold+1} ===\")\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    print(f\"Training shapes - X: {X_train.shape}, y: {y_train.shape}\")\n",
        "    print(f\"Testing shapes - X: {X_test.shape}, y: {y_test.shape}\")\n",
        "\n",
        "    # === 4. Model Training ===\n",
        "    # Input shape to U-Net is (height, width, channels)\n",
        "    model = build_unet(X_train.shape[1:])\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100, # Increased epochs\n",
        "        batch_size=8, # Adjusted batch size for U-Net\n",
        "        validation_split=0.1, # Use a small validation split from training data\n",
        "        callbacks=[\n",
        "            EarlyStopping(patience=20, restore_best_weights=True, monitor='val_loss'), # Increased patience\n",
        "            ReduceLROnPlateau(factor=0.5, patience=10, monitor='val_loss', min_lr=0.000001) # Increased patience\n",
        "        ],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # === 5. Enhanced Evaluation ===\n",
        "    print(\"\\nEvaluating model on test set...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Flatten the image data for metric calculation (needed for sklearn metrics)\n",
        "    y_test_flat = y_test.reshape(-1)\n",
        "    y_pred_flat = y_pred.reshape(-1)\n",
        "\n",
        "    # --- Regression Metrics ---\n",
        "    mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
        "    r2 = r2_score(y_test_flat, y_pred_flat)\n",
        "\n",
        "    all_mse.append(mse)\n",
        "    all_rmse.append(rmse)\n",
        "    all_mae.append(mae)\n",
        "    all_r2.append(r2)\n",
        "\n",
        "    print(f\"\\nðŸ“Š Fold {fold+1} Regression Test Metrics:\")\n",
        "    print(f\"âœ… MSE (Mean Squared Error): {mse:.6f}\")\n",
        "    print(f\"âœ… RMSE (Root Mean Squared Error): {rmse:.6f}\")\n",
        "    print(f\"âœ… MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "    print(f\"âœ… RÂ² (R-squared): {r2:.4f}\")\n",
        "\n",
        "    # --- Classification Metrics (for Demonstration Purposes Only) ---\n",
        "    print(\"\\nðŸ”„ Fold {fold+1} Classification Metrics (Derived for Demonstration - See Note Below):\")\n",
        "\n",
        "    # Define a threshold to convert regression output to binary classification.\n",
        "    # Using the median of the true test values (flattened) as a simple example threshold.\n",
        "    classification_threshold = np.median(y_test_flat)\n",
        "    print(f\"  (Using classification threshold: {classification_threshold:.4f} based on median of true test values)\")\n",
        "\n",
        "    # Convert true and predicted continuous values to binary classes based on the threshold\n",
        "    y_test_binary = (y_test_flat > classification_threshold).astype(int)\n",
        "    y_pred_binary = (y_pred_flat > classification_threshold).astype(int)\n",
        "\n",
        "    # Calculate classification metrics\n",
        "    accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "    precision = precision_score(y_test_binary, y_pred_binary, zero_division=0) # zero_division=0 handles cases with no positive predictions\n",
        "    recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "\n",
        "    all_accuracy.append(accuracy)\n",
        "    all_precision.append(precision)\n",
        "    all_recall.append(recall)\n",
        "\n",
        "    print(f\"âœ… Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"âœ… Precision: {precision:.4f}\")\n",
        "    print(f\"âœ… Recall:    {recall:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Important Note on Classification Metrics ---\")\n",
        "    print(\"The 'Accuracy', 'Precision', and 'Recall' metrics above are calculated by converting the continuous regression \"\n",
        "          \"outputs into binary classes using an arbitrary threshold (the median of true test values in this case).\")\n",
        "    print(\"This transformation is done *only for demonstration* of these metrics. Your model's primary task is regression, \"\n",
        "          \"and its performance should be primarily judged by MSE, RMSE, MAE, and R-squared.\")\n",
        "    print(\"Choosing a different threshold would likely change these classification metric values.\")\n",
        "\n",
        "    # === 6. Enhanced Visualization ===\n",
        "    print(\"\\nGenerating plots for visualization...\")\n",
        "    plt.figure(figsize=(18, 12)) # Larger figure for multiple plots\n",
        "\n",
        "    # Plot 1: True vs Predicted Plot (flattened)\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.scatter(y_test_flat, y_pred_flat, alpha=0.1, s=5) # Reduced alpha and size for dense scatter\n",
        "    plt.plot([y_test_flat.min(), y_test_flat.max()], [y_test_flat.min(), y_test_flat.max()], 'r--', label='Perfect Prediction')\n",
        "    plt.title(f\"Fold {fold+1}: True vs Predicted Values\\n(RÂ²: {r2:.4f})\", fontsize=14)\n",
        "    plt.xlabel(\"True Values\", fontsize=12)\n",
        "    plt.ylabel(\"Predictions\", fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    # plt.axis('equal') # Ensure aspect ratio is equal - Removed as it distorts view with varied data ranges\n",
        "\n",
        "    # Plot 2: Error Distribution\n",
        "    plt.subplot(2, 3, 2)\n",
        "    errors = y_test_flat - y_pred_flat\n",
        "    plt.hist(errors, bins=50, color='skyblue', edgecolor='black')\n",
        "    plt.title(f\"Fold {fold+1}: Prediction Error Distribution\\n(MAE: {mae:.4f})\", fontsize=14)\n",
        "    plt.xlabel(\"Prediction Error (True - Predicted)\", fontsize=12)\n",
        "    plt.ylabel(\"Frequency\", fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 3: Sample True Salinity Map\n",
        "    plt.subplot(2, 3, 3)\n",
        "    if y_test.shape[0] > 0: # Ensure there's at least one sample\n",
        "        plt.imshow(y_test[0,:,:,0], cmap='viridis', origin='lower')\n",
        "        plt.title(f\"Fold {fold+1}: Sample True Map\", fontsize=14)\n",
        "        plt.colorbar(label=\"Standardized Salinity\")\n",
        "        plt.xlabel(\"Longitude Index\")\n",
        "        plt.ylabel(\"Latitude Index\")\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No test data for sample map\", horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
        "\n",
        "\n",
        "    # Plot 4: Sample Predicted Salinity Map\n",
        "    plt.subplot(2, 3, 4)\n",
        "    if y_pred.shape[0] > 0: # Ensure there's at least one sample\n",
        "        plt.imshow(y_pred[0,:,:,0], cmap='viridis', origin='lower')\n",
        "        plt.title(f\"Fold {fold+1}: Sample Predicted Map\", fontsize=14)\n",
        "        plt.colorbar(label=\"Standardized Salinity\")\n",
        "        plt.xlabel(\"Longitude Index\")\n",
        "        plt.ylabel(\"Latitude Index\")\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No test data for sample map\", horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
        "\n",
        "\n",
        "    # Plot 5: Sample Error Map (True - Predicted)\n",
        "    plt.subplot(2, 3, 5)\n",
        "    if y_test.shape[0] > 0 and y_pred.shape[0] > 0: # Ensure samples exist\n",
        "        error_map = y_test[0,:,:,0] - y_pred[0,:,:,0]\n",
        "        plt.imshow(error_map, cmap='coolwarm', vmin=-1.0, vmax=1.0, origin='lower') # Set vmin/vmax for consistent color scaling\n",
        "        plt.title(f\"Fold {fold+1}: Sample Error Map (True - Pred)\", fontsize=14)\n",
        "        plt.colorbar(label=\"Standardized Error\")\n",
        "        plt.xlabel(\"Longitude Index\")\n",
        "        plt.ylabel(\"Latitude Index\")\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No test data for sample error map\", horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
        "\n",
        "    plt.tight_layout(pad=3.0)\n",
        "    plt.show()\n",
        "\n",
        "# After all folds, print average metrics\n",
        "print(\"\\n=== Average Metrics Across All Folds ===\")\n",
        "print(f\"Average MSE: {np.mean(all_mse):.6f}\")\n",
        "print(f\"Average RMSE: {np.mean(all_rmse):.6f}\")\n",
        "print(f\"Average MAE: {np.mean(all_mae):.6f}\")\n",
        "print(f\"Average RÂ²: {np.mean(all_r2):.4f}\")\n",
        "print(f\"Average Accuracy: {np.mean(all_accuracy):.4f}\")\n",
        "print(f\"Average Precision: {np.mean(all_precision):.4f}\")\n",
        "print(f\"Average Recall: {np.mean(all_recall):.4f}\")\n",
        "\n",
        "print(\"\\nScript execution complete. Please check the console output for metrics and the plots for visualization.\")"
      ],
      "metadata": {
        "id": "vCpdZ8u4Oz-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU"
      ],
      "metadata": {
        "id": "reLRACNKO3Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xarray as xr\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                             accuracy_score, precision_score, recall_score, confusion_matrix)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout # Changed LSTM to GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "# === 1. Data Loading and Preprocessing ===\n",
        "def load_time_series_data(filepath, downsample_factor=4):\n",
        "    \"\"\"\n",
        "    Load and preprocess time series data from NetCDF file.\n",
        "    Extracts the 'SALT' variable, performs downsampling, flipping,\n",
        "    imputes missing values spatially, and then computes the mean salinity\n",
        "    to form a 1D time series for LSTM input. Finally, standardizes this series.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the NetCDF file.\n",
        "        downsample_factor (int): Factor by which to downsample the spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (time_series, scaler) where time_series is the preprocessed 1D array,\n",
        "               and scaler is the StandardScaler object used for inverse transformation.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Dataset file not found at '{filepath}'\")\n",
        "        print(\"Please ensure the 'salinity.nc' file is in the specified path.\")\n",
        "        raise FileNotFoundError(f\"Dataset file not found at {filepath}\")\n",
        "\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    ds = xr.open_dataset(filepath, decode_times=False)\n",
        "\n",
        "    # Extract 'SALT' variable, take the first depth layer, and convert to numpy array\n",
        "    salt_data = ds['SALT'].values[:, 0, :, :] # Remove depth dimension\n",
        "\n",
        "    print(f\"Original SALT data shape: {salt_data.shape}\")\n",
        "\n",
        "    # Downsample and flip vertically to correct orientation\n",
        "    salt_data = salt_data[:, ::downsample_factor, ::downsample_factor]\n",
        "    salt_data = np.flip(salt_data, axis=1)\n",
        "\n",
        "    print(f\"Downsampled and flipped SALT data shape: {salt_data.shape}\")\n",
        "\n",
        "    # --- IMPORTANT FIX: Impute missing values on the 3D spatial data first ---\n",
        "    # This ensures that each spatial slice has no NaNs before taking the mean.\n",
        "    print(\"Imputing missing values in 3D SALT data using median strategy per slice...\")\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    for i in range(salt_data.shape[0]):\n",
        "        # Reshape each 2D slice for imputation, then reshape back\n",
        "        # Ensure there's at least one non-NaN value in the slice for median strategy to work.\n",
        "        # If an entire slice is NaN, the median imputation might still fail or yield NaN.\n",
        "        # However, for oceanographic data, usually not all values in a large area are NaN.\n",
        "        # Handle cases where an entire slice might be NaN (rare but possible).\n",
        "        if np.all(np.isnan(salt_data[i])):\n",
        "            print(f\"Warning: Entire slice {i} is NaN. Median imputation might not be effective.\")\n",
        "            # Option: Fill with global median/mean if slice is all NaN\n",
        "            # For now, SimpleImputer will handle it by skipping the feature if no valid values.\n",
        "            # To be safer, you might consider filling with a very small constant or zero if this happens frequently.\n",
        "            pass # Let SimpleImputer try, it will warn if it can't\n",
        "\n",
        "        imputed_slice = imputer.fit_transform(salt_data[i].reshape(-1, 1)).reshape(salt_data[i].shape)\n",
        "        salt_data[i] = imputed_slice\n",
        "    print(\"Missing values imputed in 3D data.\")\n",
        "\n",
        "    # Compute the mean salinity for each time step to get a 1D time series\n",
        "    time_series = np.nanmean(salt_data, axis=(1, 2)) # Use nanmean to handle potential remaining NaNs\n",
        "\n",
        "    # Reshape to 2D (time steps, features) for StandardScaler\n",
        "    if len(time_series.shape) == 1:\n",
        "        time_series = time_series.reshape(-1, 1)\n",
        "\n",
        "    print(f\"Time series shape after mean aggregation: {time_series.shape}\")\n",
        "    print(f\"Time series shape BEFORE standardization: {time_series.shape}\") # Debug print\n",
        "\n",
        "    # Standardize the data\n",
        "    print(\"Standardizing time series data...\")\n",
        "    scaler = StandardScaler()\n",
        "    # Check if time_series is empty or contains only NaNs before scaling\n",
        "    if time_series.shape[1] == 0 or np.all(np.isnan(time_series)):\n",
        "        raise ValueError(\"Time series data is empty or contains only NaNs after preprocessing, cannot standardize.\")\n",
        "    time_series = scaler.fit_transform(time_series)\n",
        "    print(\"Data standardization complete.\")\n",
        "\n",
        "    return time_series, scaler\n",
        "\n",
        "# === 2. GRU Model Architecture === # Renamed function\n",
        "def create_gru_model(input_shape, units=[100, 80, 60], dropout_rate=0.2): # Renamed function\n",
        "    \"\"\"\n",
        "    Creates a Sequential GRU model for time series prediction.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input data (seq_length, n_features).\n",
        "        units (list): List of integers specifying the number of GRU units for each layer.\n",
        "        dropout_rate (float): Dropout rate for regularization.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.models.Sequential: Compiled GRU model.\n",
        "    \"\"\"\n",
        "    print(\"\\nBuilding GRU model...\") # Updated print statement\n",
        "    model = Sequential()\n",
        "\n",
        "    # First GRU layer # Changed LSTM to GRU\n",
        "    model.add(GRU(units[0], activation='tanh', # Reverted to tanh for GRU layers\n",
        "                   input_shape=input_shape,\n",
        "                   return_sequences=True))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Second GRU layer # Changed LSTM to GRU\n",
        "    model.add(GRU(units[1], activation='tanh', # Reverted to tanh for GRU layers\n",
        "                   return_sequences=True))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Third GRU layer # Changed LSTM to GRU\n",
        "    model.add(GRU(units[2], activation='tanh')) # Reverted to tanh for GRU layers\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer for regression (single continuous value prediction)\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    print(\"GRU model built and compiled.\") # Updated print statement\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# === 3. Sequence Creation ===\n",
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"\n",
        "    Creates input sequences (X) and corresponding target values (y)\n",
        "    for time series forecasting.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): The 1D or 2D time series data.\n",
        "        seq_length (int): The number of past time steps to use as input\n",
        "                          to predict the next time step.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y) where X are the input sequences and y are the targets.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# === 4. Main Execution ===\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    FILE_PATH = \"/content/drive/MyDrive/salinity.nc\"\n",
        "    SEQ_LENGTH = 30\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 200\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        time_series_data, scaler = load_time_series_data(FILE_PATH)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        print(\"Exiting program. Please provide the correct dataset path.\")\n",
        "        exit()\n",
        "    except ValueError as e:\n",
        "        print(f\"Data preprocessing failed: {e}\")\n",
        "        print(\"Exiting program.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "    if time_series_data is not None:\n",
        "        # Create sequences\n",
        "        X, y = create_sequences(time_series_data, SEQ_LENGTH)\n",
        "\n",
        "        # Train-test split (maintaining temporal order)\n",
        "        split_idx = int(0.8 * len(X))\n",
        "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        print(f\"\\nTraining data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "        print(f\"Test data shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "\n",
        "        # Create GRU model # Changed function call\n",
        "        model = create_gru_model(input_shape=(SEQ_LENGTH, 1))\n",
        "\n",
        "        # Callbacks\n",
        "        early_stop = EarlyStopping(monitor='val_loss',\n",
        "                                   patience=15,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "        # Train model\n",
        "        print(\"\\nStarting model training...\")\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[early_stop],\n",
        "            verbose=1\n",
        "        )\n",
        "        print(\"Model training finished.\")\n",
        "\n",
        "        # Evaluation\n",
        "        print(\"\\nEvaluating model on test set...\")\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Inverse transform to get original scale values for meaningful metrics\n",
        "        y_test_orig = scaler.inverse_transform(y_test)\n",
        "        y_pred_orig = scaler.inverse_transform(y_pred)\n",
        "\n",
        "        # --- Regression Metrics ---\n",
        "        mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
        "        r2 = r2_score(y_test_orig, y_pred_orig)\n",
        "\n",
        "        print(\"\\nðŸ“Š Final Regression Test Metrics:\")\n",
        "        print(f\"âœ… MSE (Mean Squared Error): {mse:.6f}\")\n",
        "        print(f\"âœ… RMSE (Root Mean Squared Error): {rmse:.6f}\")\n",
        "        print(f\"âœ… MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "        print(f\"âœ… RÂ² (R-squared): {r2:.4f}\")\n",
        "\n",
        "        # --- Classification Metrics (for Demonstration Purposes Only) ---\n",
        "        print(\"\\nðŸ”„ Classification Metrics (Derived for Demonstration - See Note Below):\")\n",
        "\n",
        "        classification_threshold = np.median(y_test_orig)\n",
        "        print(f\" Â (Using classification threshold: {classification_threshold:.4f} based on median of true test values)\")\n",
        "\n",
        "        y_test_binary = (y_test_orig > classification_threshold).astype(int)\n",
        "        y_pred_binary = (y_pred_orig > classification_threshold).astype(int)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "        precision = precision_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "        recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "        conf_matrix = confusion_matrix(y_test_binary, y_pred_binary)\n",
        "\n",
        "        print(f\"âœ… Accuracy: Â {accuracy:.4f}\")\n",
        "        print(f\"âœ… Precision: {precision:.4f}\")\n",
        "        print(f\"âœ… Recall: Â  Â {recall:.4f}\")\n",
        "        print(f\"âœ… Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Important Note on Classification Metrics ---\")\n",
        "        print(\"The 'Accuracy', 'Precision', and 'Recall' metrics above are calculated by converting the continuous regression \"\n",
        "              \"outputs into binary classes using an arbitrary threshold (the median of true test values in this case).\")\n",
        "        print(\"This transformation is done *only for demonstration* of these metrics. Your model's primary task is regression, \"\n",
        "              \"and its performance should be primarily judged by MSE, RMSE, MAE, and R-squared.\")\n",
        "        print(\"Choosing a different threshold would likely change these classification metric values.\")\n",
        "\n",
        "        # === 5. Visualization ===\n",
        "        print(\"\\nGenerating plots for visualization...\")\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        # Subplot 1: True vs. Predicted Values Plot (Time Series)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(y_test_orig, label='True Mean Salinity', color='blue', linestyle='-')\n",
        "        plt.plot(y_pred_orig, label='Predicted Mean Salinity', color='red', linestyle='--')\n",
        "        plt.title(\"True vs. Predicted Mean Salinity (Original Scale)\")\n",
        "        plt.xlabel(\"Time Step Index\")\n",
        "        plt.ylabel(\"Mean Salinity\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Subplot 2: Loss History\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss History (MSE)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Separate plot for Confusion Matrix\n",
        "        plt.figure(figsize=(7, 6))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "                    yticklabels=['Actual 0', 'Actual 1'])\n",
        "        plt.title('Confusion Matrix for Binary Classification')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nScript execution complete. Check console for metrics and plots for visualization.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Data loading failed. Cannot proceed with model training.\")\n"
      ],
      "metadata": {
        "id": "dYi7qmFCO410"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "65SoylS5Pg-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xarray as xr\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                             accuracy_score, precision_score, recall_score, confusion_matrix)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "# === 1. Data Loading and Preprocessing ===\n",
        "def load_time_series_data(filepath, downsample_factor=4): # Removed time_dim, var_name as they were not consistently used for SALT\n",
        "    \"\"\"\n",
        "    Load and preprocess time series data from NetCDF file.\n",
        "    Extracts the 'SALT' variable, performs downsampling, flipping,\n",
        "    imputes missing values spatially, and then computes the mean salinity\n",
        "    to form a 1D time series for LSTM input. Finally, standardizes this series.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the NetCDF file.\n",
        "        downsample_factor (int): Factor by which to downsample the spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (time_series, scaler) where time_series is the preprocessed 1D array,\n",
        "               and scaler is the StandardScaler object used for inverse transformation.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Dataset file not found at '{filepath}'\")\n",
        "        print(\"Please ensure the 'salinity.nc' file is in the specified path.\")\n",
        "        raise FileNotFoundError(f\"Dataset file not found at {filepath}\")\n",
        "\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    ds = xr.open_dataset(filepath, decode_times=False)\n",
        "\n",
        "    # Extract 'SALT' variable, take the first depth layer, and convert to numpy array\n",
        "    salt_data = ds['SALT'].values[:, 0, :, :] # Remove depth dimension\n",
        "\n",
        "    print(f\"Original SALT data shape: {salt_data.shape}\")\n",
        "\n",
        "    # Downsample and flip vertically to correct orientation\n",
        "    salt_data = salt_data[:, ::downsample_factor, ::downsample_factor]\n",
        "    salt_data = np.flip(salt_data, axis=1)\n",
        "\n",
        "    print(f\"Downsampled and flipped SALT data shape: {salt_data.shape}\")\n",
        "\n",
        "    # --- IMPORTANT FIX: Impute missing values on the 3D spatial data first ---\n",
        "    # This ensures that each spatial slice has no NaNs before taking the mean.\n",
        "    print(\"Imputing missing values in 3D SALT data using median strategy per slice...\")\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    for i in range(salt_data.shape[0]):\n",
        "        # Reshape each 2D slice for imputation, then reshape back\n",
        "        # Ensure there's at least one non-NaN value in the slice for median strategy to work.\n",
        "        # If an entire slice is NaN, the median imputation might still fail or yield NaN.\n",
        "        # However, for oceanographic data, usually not all values in a large area are NaN.\n",
        "        imputed_slice = imputer.fit_transform(salt_data[i].reshape(-1, 1)).reshape(salt_data[i].shape)\n",
        "        salt_data[i] = imputed_slice\n",
        "    print(\"Missing values imputed in 3D data.\")\n",
        "\n",
        "    # Compute the mean salinity for each time step to get a 1D time series\n",
        "    time_series = np.mean(salt_data, axis=(1, 2))\n",
        "\n",
        "    # Reshape to 2D (time steps, features) for StandardScaler\n",
        "    if len(time_series.shape) == 1:\n",
        "        time_series = time_series.reshape(-1, 1)\n",
        "\n",
        "    print(f\"Time series shape after mean aggregation: {time_series.shape}\")\n",
        "\n",
        "    # Standardize the data\n",
        "    print(\"Standardizing time series data...\")\n",
        "    scaler = StandardScaler()\n",
        "    # Check if time_series is empty or contains only NaNs before scaling\n",
        "    if time_series.shape[1] == 0 or np.all(np.isnan(time_series)):\n",
        "        raise ValueError(\"Time series data is empty or contains only NaNs after preprocessing, cannot standardize.\")\n",
        "    time_series = scaler.fit_transform(time_series)\n",
        "    print(\"Data standardization complete.\")\n",
        "\n",
        "    return time_series, scaler\n",
        "\n",
        "# === 2. LSTM Model Architecture ===\n",
        "def create_lstm_model(input_shape, units=[100, 80, 60], dropout_rate=0.2):\n",
        "    \"\"\"\n",
        "    Creates a Sequential LSTM model for time series prediction.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input data (seq_length, n_features).\n",
        "        units (list): List of integers specifying the number of LSTM units for each layer.\n",
        "        dropout_rate (float): Dropout rate for regularization.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.models.Sequential: Compiled LSTM model.\n",
        "    \"\"\"\n",
        "    print(\"\\nBuilding LSTM model...\")\n",
        "    model = Sequential()\n",
        "\n",
        "    # First LSTM layer\n",
        "    model.add(LSTM(units[0], activation='relu',\n",
        "                   input_shape=input_shape,\n",
        "                   return_sequences=True))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Second LSTM layer\n",
        "    model.add(LSTM(units[1], activation='relu',\n",
        "                   return_sequences=True))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Third LSTM layer\n",
        "    model.add(LSTM(units[2], activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer for regression (single continuous value prediction)\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    print(\"LSTM model built and compiled.\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# === 3. Sequence Creation ===\n",
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"\n",
        "    Creates input sequences (X) and corresponding target values (y)\n",
        "    for time series forecasting.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): The 1D or 2D time series data.\n",
        "        seq_length (int): The number of past time steps to use as input\n",
        "                          to predict the next time step.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y) where X are the input sequences and y are the targets.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# === 4. Main Execution ===\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    FILE_PATH = \"/content/drive/MyDrive/salinity.nc\"\n",
        "    SEQ_LENGTH = 30\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 200\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        time_series_data, scaler = load_time_series_data(FILE_PATH)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        print(\"Exiting program. Please provide the correct dataset path.\")\n",
        "        exit()\n",
        "    except ValueError as e:\n",
        "        print(f\"Data preprocessing failed: {e}\")\n",
        "        print(\"Exiting program.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "    if time_series_data is not None:\n",
        "        # Create sequences\n",
        "        X, y = create_sequences(time_series_data, SEQ_LENGTH)\n",
        "\n",
        "        # Train-test split (maintaining temporal order)\n",
        "        split_idx = int(0.8 * len(X))\n",
        "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        print(f\"\\nTraining data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "        print(f\"Test data shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "\n",
        "        # Create LSTM model\n",
        "        model = create_lstm_model(input_shape=(SEQ_LENGTH, 1))\n",
        "\n",
        "        # Callbacks\n",
        "        early_stop = EarlyStopping(monitor='val_loss',\n",
        "                                   patience=15,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "        # Train model\n",
        "        print(\"\\nStarting model training...\")\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[early_stop],\n",
        "            verbose=1\n",
        "        )\n",
        "        print(\"Model training finished.\")\n",
        "\n",
        "        # Evaluation\n",
        "        print(\"\\nEvaluating model on test set...\")\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Inverse transform to get original scale values for meaningful metrics\n",
        "        y_test_orig = scaler.inverse_transform(y_test)\n",
        "        y_pred_orig = scaler.inverse_transform(y_pred)\n",
        "\n",
        "        # --- Regression Metrics ---\n",
        "        mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
        "        r2 = r2_score(y_test_orig, y_pred_orig)\n",
        "\n",
        "        print(\"\\nðŸ“Š Final Regression Test Metrics:\")\n",
        "        print(f\"âœ… MSE (Mean Squared Error): {mse:.6f}\")\n",
        "        print(f\"âœ… RMSE (Root Mean Squared Error): {rmse:.6f}\")\n",
        "        print(f\"âœ… MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "        print(f\"âœ… RÂ² (R-squared): {r2:.4f}\")\n",
        "\n",
        "        # --- Classification Metrics (for Demonstration Purposes Only) ---\n",
        "        print(\"\\nðŸ”„ Classification Metrics (Derived for Demonstration - See Note Below):\")\n",
        "\n",
        "        classification_threshold = np.median(y_test_orig)\n",
        "        print(f\" Â (Using classification threshold: {classification_threshold:.4f} based on median of true test values)\")\n",
        "\n",
        "        y_test_binary = (y_test_orig > classification_threshold).astype(int)\n",
        "        y_pred_binary = (y_pred_orig > classification_threshold).astype(int)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "        precision = precision_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "        recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "        conf_matrix = confusion_matrix(y_test_binary, y_pred_binary)\n",
        "\n",
        "        print(f\"âœ… Accuracy: Â {accuracy:.4f}\")\n",
        "        print(f\"âœ… Precision: {precision:.4f}\")\n",
        "        print(f\"âœ… Recall: Â  Â {recall:.4f}\")\n",
        "        print(f\"âœ… Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Important Note on Classification Metrics ---\")\n",
        "        print(\"The 'Accuracy', 'Precision', and 'Recall' metrics above are calculated by converting the continuous regression \"\n",
        "              \"outputs into binary classes using an arbitrary threshold (the median of true test values in this case).\")\n",
        "        print(\"This transformation is done *only for demonstration* of these metrics. Your model's primary task is regression, \"\n",
        "              \"and its performance should be primarily judged by MSE, RMSE, MAE, and R-squared.\")\n",
        "        print(\"Choosing a different threshold would likely change these classification metric values.\")\n",
        "\n",
        "        # === 5. Visualization ===\n",
        "        print(\"\\nGenerating plots for visualization...\")\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        # Subplot 1: True vs. Predicted Values Plot (Time Series)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(y_test_orig, label='True Mean Salinity', color='blue', linestyle='-')\n",
        "        plt.plot(y_pred_orig, label='Predicted Mean Salinity', color='red', linestyle='--')\n",
        "        plt.title(\"True vs. Predicted Mean Salinity (Original Scale)\")\n",
        "        plt.xlabel(\"Time Step Index\")\n",
        "        plt.ylabel(\"Mean Salinity\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Subplot 2: Loss History\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss History (MSE)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Separate plot for Confusion Matrix\n",
        "        plt.figure(figsize=(7, 6))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "                    yticklabels=['Actual 0', 'Actual 1'])\n",
        "        plt.title('Confusion Matrix for Binary Classification')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nScript execution complete. Check console for metrics and plots for visualization.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Data loading failed. Cannot proceed with model training.\")\n"
      ],
      "metadata": {
        "id": "aW-vvG8MPiCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN"
      ],
      "metadata": {
        "id": "BVEN2IdJPjvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xarray as xr\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error, # Added mean_absolute_error\n",
        "                             accuracy_score, precision_score, recall_score, confusion_matrix) # Added classification metrics and confusion_matrix\n",
        "from tensorflow.keras.models import Sequential # Using tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns # Added for confusion matrix visualization\n",
        "\n",
        "# === 1. Data Loading and Preprocessing ===\n",
        "def load_time_series_data(filepath, downsample_factor=4):\n",
        "    \"\"\"\n",
        "    Load and preprocess time series data from NetCDF file.\n",
        "    Extracts the 'SALT' variable, performs downsampling, flipping,\n",
        "    imputes missing values spatially, and then computes the mean salinity\n",
        "    to form a 1D time series for ANN input. Finally, standardizes this series.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the NetCDF file.\n",
        "        downsample_factor (int): Factor by which to downsample the spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (time_series, scaler) where time_series is the preprocessed 1D array,\n",
        "               and scaler is the StandardScaler object used for inverse transformation.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Dataset file not found at '{filepath}'\")\n",
        "        print(\"Please ensure the 'salinity.nc' file is in the specified path.\")\n",
        "        raise FileNotFoundError(f\"Dataset file not found at {filepath}\")\n",
        "\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    ds = xr.open_dataset(filepath, decode_times=False)\n",
        "\n",
        "    # Extract 'SALT' variable, take the first depth layer, and convert to numpy array\n",
        "    salt_data = ds['SALT'].values[:, 0, :, :] # Remove depth dimension\n",
        "\n",
        "    print(f\"Original SALT data shape: {salt_data.shape}\")\n",
        "\n",
        "    # Downsample and flip vertically to correct orientation\n",
        "    salt_data = salt_data[:, ::downsample_factor, ::downsample_factor]\n",
        "    salt_data = np.flip(salt_data, axis=1)\n",
        "\n",
        "    print(f\"Downsampled and flipped SALT data shape: {salt_data.shape}\")\n",
        "\n",
        "    # Impute missing values on the 3D spatial data first\n",
        "    print(\"Imputing missing values in 3D SALT data using median strategy per slice...\")\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    for i in range(salt_data.shape[0]):\n",
        "        if np.all(np.isnan(salt_data[i])):\n",
        "            print(f\"Warning: Entire slice {i} is NaN. Median imputation might not be effective.\")\n",
        "        imputed_slice = imputer.fit_transform(salt_data[i].reshape(-1, 1)).reshape(salt_data[i].shape)\n",
        "        salt_data[i] = imputed_slice\n",
        "    print(\"Missing values imputed in 3D data.\")\n",
        "\n",
        "    # Compute the mean salinity for each time step to get a 1D time series\n",
        "    time_series = np.nanmean(salt_data, axis=(1, 2)) # Use nanmean to handle potential remaining NaNs\n",
        "\n",
        "    # Reshape to 2D (time steps, features) for StandardScaler\n",
        "    if len(time_series.shape) == 1:\n",
        "        time_series = time_series.reshape(-1, 1)\n",
        "\n",
        "    print(f\"Time series shape after mean aggregation: {time_series.shape}\")\n",
        "    print(f\"Time series shape BEFORE standardization: {time_series.shape}\") # Debug print\n",
        "\n",
        "    # Standardize the data\n",
        "    print(\"Standardizing time series data...\")\n",
        "    scaler = StandardScaler()\n",
        "    if time_series.shape[1] == 0 or np.all(np.isnan(time_series)):\n",
        "        raise ValueError(\"Time series data is empty or contains only NaNs after preprocessing, cannot standardize.\")\n",
        "    time_series = scaler.fit_transform(time_series)\n",
        "    print(\"Data standardization complete.\")\n",
        "\n",
        "    return time_series, scaler\n",
        "\n",
        "# === 2. ANN Model Architecture ===\n",
        "def create_ann_model(input_shape, units=[256, 128, 64], dropout_rate=0.3, l2_reg=0.01):\n",
        "    \"\"\"\n",
        "    Creates a Sequential ANN model for time series prediction.\n",
        "\n",
        "    Args:\n",
        "        input_shape (int): Number of features in the input (window size).\n",
        "        units (list): List of integers specifying the number of units for each Dense layer.\n",
        "        dropout_rate (float): Dropout rate for regularization.\n",
        "        l2_reg (float): L2 regularization factor.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.models.Sequential: Compiled ANN model.\n",
        "    \"\"\"\n",
        "    print(\"\\nBuilding ANN model...\")\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Dense(units[0], activation='relu',\n",
        "                    input_dim=input_shape, # For ANN, input_dim is the flattened sequence length\n",
        "                    kernel_regularizer=l2(l2_reg)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Hidden layers\n",
        "    model.add(Dense(units[1], activation='relu',\n",
        "                    kernel_regularizer=l2(l2_reg)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(units[2], activation='relu',\n",
        "                    kernel_regularizer=l2(l2_reg)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer (linear activation for regression)\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae']) # Added MAE metric\n",
        "    print(\"ANN model built and compiled.\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# === 3. Modified Sequence Creation for ANN ===\n",
        "def create_ann_sequences(data, window_size):\n",
        "    \"\"\"\n",
        "    Create sliding windows for ANN (flattened sequences).\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): The 1D or 2D time series data.\n",
        "        window_size (int): The number of past time steps to use as input\n",
        "                           to predict the next time step.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y) where X are the flattened input windows and y are the targets.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i+window_size].flatten())  # Flatten the window\n",
        "        y.append(data[i+window_size])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# === 4. Main Execution ===\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    FILE_PATH = \"/content/drive/MyDrive/salinity.nc\"\n",
        "    WINDOW_SIZE = 20\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 200\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        time_series_data, scaler = load_time_series_data(FILE_PATH)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        print(\"Exiting program. Please provide the correct dataset path.\")\n",
        "        exit()\n",
        "    except ValueError as e:\n",
        "        print(f\"Data preprocessing failed: {e}\")\n",
        "        print(\"Exiting program.\")\n",
        "        exit()\n",
        "\n",
        "    if time_series_data is not None:\n",
        "        # Create sequences (flattened for ANN)\n",
        "        X, y = create_ann_sequences(time_series_data, WINDOW_SIZE)\n",
        "\n",
        "        # Train-test split (maintaining temporal order for time series data)\n",
        "        split_idx = int(0.8 * len(X))\n",
        "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        print(f\"\\nTraining data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "        print(f\"Test data shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "\n",
        "        # Create ANN model\n",
        "        # Input shape for ANN is the flattened sequence length (X_train.shape[1])\n",
        "        model = create_ann_model(input_shape=X_train.shape[1])\n",
        "\n",
        "        # Callbacks\n",
        "        early_stop = EarlyStopping(monitor='val_loss',\n",
        "                                   patience=20, # Increased patience as per original snippet\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "        # Train model\n",
        "        print(\"\\nStarting model training...\")\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[early_stop],\n",
        "            verbose=1\n",
        "        )\n",
        "        print(\"Model training finished.\")\n",
        "\n",
        "        # Evaluation\n",
        "        print(\"\\nEvaluating model on test set...\")\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Inverse transform to get original scale values for meaningful metrics\n",
        "        # Ensure y_test is correctly reshaped for inverse_transform if it's 1D\n",
        "        y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "        y_pred_orig = scaler.inverse_transform(y_pred) # y_pred is already 2D from model.predict\n",
        "\n",
        "        # --- Regression Metrics ---\n",
        "        mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test_orig, y_pred_orig) # Calculate MAE\n",
        "        r2 = r2_score(y_test_orig, y_pred_orig)\n",
        "\n",
        "        print(\"\\nðŸ“Š Final Regression Test Metrics:\")\n",
        "        print(f\"âœ… MSE (Mean Squared Error): {mse:.6f}\")\n",
        "        print(f\"âœ… RMSE (Root Mean Squared Error): {rmse:.6f}\")\n",
        "        print(f\"âœ… MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "        print(f\"âœ… RÂ² (R-squared): {r2:.4f}\")\n",
        "\n",
        "        # --- Classification Metrics (for Demonstration Purposes Only) ---\n",
        "        print(\"\\nðŸ”„ Classification Metrics (Derived for Demonstration - See Note Below):\")\n",
        "\n",
        "        # Define a threshold to convert regression output to binary classification.\n",
        "        # Using the median of the true test values as a simple example threshold.\n",
        "        classification_threshold = np.median(y_test_orig)\n",
        "        print(f\" Â (Using classification threshold: {classification_threshold:.4f} based on median of true test values)\")\n",
        "\n",
        "        # Convert true and predicted continuous values to binary classes based on the threshold\n",
        "        y_test_binary = (y_test_orig > classification_threshold).astype(int)\n",
        "        y_pred_binary = (y_pred_orig > classification_threshold).astype(int)\n",
        "\n",
        "        # Calculate classification metrics\n",
        "        accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "        precision = precision_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "        recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "        conf_matrix = confusion_matrix(y_test_binary, y_pred_binary)\n",
        "\n",
        "        print(f\"âœ… Accuracy: Â {accuracy:.4f}\")\n",
        "        print(f\"âœ… Precision: {precision:.4f}\")\n",
        "        print(f\"âœ… Recall: Â  Â {recall:.4f}\")\n",
        "        print(f\"âœ… Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Important Note on Classification Metrics ---\")\n",
        "        print(\"The 'Accuracy', 'Precision', and 'Recall' metrics above are calculated by converting the continuous regression \"\n",
        "              \"outputs into binary classes using an arbitrary threshold (the median of true test values in this case).\")\n",
        "        print(\"This transformation is done *only for demonstration* of these metrics. Your model's primary task is regression, \"\n",
        "              \"and its performance should be primarily judged by MSE, RMSE, MAE, and R-squared.\")\n",
        "        print(\"Choosing a different threshold would likely change these classification metric values.\")\n",
        "\n",
        "        # === 5. Visualization ===\n",
        "        print(\"\\nGenerating plots for visualization...\")\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        # Subplot 1: True vs. Predicted Values Plot (Time Series)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        # Ensure y_test_orig and y_pred_orig are 1D arrays for plotting\n",
        "        plt.plot(y_test_orig.flatten(), label='True Mean Salinity', color='blue', linestyle='-')\n",
        "        plt.plot(y_pred_orig.flatten(), label='Predicted Mean Salinity', color='red', linestyle='--')\n",
        "        plt.title(\"True vs. Predicted Mean Salinity (Original Scale)\")\n",
        "        plt.xlabel(\"Time Step Index\")\n",
        "        plt.ylabel(\"Mean Salinity\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Subplot 2: Loss History\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss History (MSE)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Separate figure for Scatter Plot and Confusion Matrix\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        # Subplot 1 of second figure: Scatter plot of actual vs predicted values (as in your original snippet)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.scatter(y_test_orig, y_pred_orig, alpha=0.6, color='royalblue', edgecolor='white', label='Predictions')\n",
        "        max_val = max(y_test_orig.max(), y_pred_orig.max())\n",
        "        min_val = min(y_test_orig.min(), y_pred_orig.min())\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal Prediction')\n",
        "        plt.annotate(f'$R^2$ = {r2:.4f}',\n",
        "                     xy=(0.05, 0.85),\n",
        "                     xycoords='axes fraction',\n",
        "                     fontsize=14,\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "        plt.title('ANN Model Performance: Actual vs Predicted Salinity', fontsize=14, pad=20)\n",
        "        plt.xlabel('Actual Salinity Values', fontsize=12)\n",
        "        plt.ylabel('Predicted Salinity Values', fontsize=12)\n",
        "        plt.legend(fontsize=12, loc='upper left')\n",
        "        plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "\n",
        "        # Subplot 2 of second figure: Confusion Matrix\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "                    yticklabels=['Actual 0', 'Actual 1'])\n",
        "        plt.title('Confusion Matrix for Binary Classification')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nScript execution complete. Check console for metrics and plots for visualization.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Data loading failed. Cannot proceed with model training.\")\n"
      ],
      "metadata": {
        "id": "3XPgfsbcPlXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAM"
      ],
      "metadata": {
        "id": "zgpKTdR7Pnsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                             accuracy_score, precision_score, recall_score, confusion_matrix)\n",
        "from pygam import LinearGAM, s, f\n",
        "from pygam.terms import TermList\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === 1. Data Loading and Preprocessing for GAM ===\n",
        "def load_and_prepare_data(filepath, n_lags=5):\n",
        "    \"\"\"\n",
        "    Load and preprocess salinity data for GAM.\n",
        "    Extracts 'SALT' variable, computes its mean time series,\n",
        "    imputes missing values, standardizes, creates lag features from salinity,\n",
        "    and adds time-based features (day of year, month).\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the NetCDF file.\n",
        "        n_lags (int): Number of lag features to create.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y, scaler) where X is the feature DataFrame,\n",
        "               y is the target array, and scaler is the StandardScaler for salinity.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Dataset file not found at '{filepath}'\")\n",
        "        print(\"Please ensure the 'salinity.nc' file is in the specified path.\")\n",
        "        raise FileNotFoundError(f\"Dataset file not found at {filepath}\")\n",
        "\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    ds = xr.open_dataset(filepath, decode_times=False)\n",
        "\n",
        "    # Extract 'SALT' variable, take the first depth layer\n",
        "    # Assuming 'SALT' has dimensions (time, depth, lat, lon)\n",
        "    if 'SALT' not in ds:\n",
        "        raise ValueError(\"'SALT' variable not found in the dataset.\")\n",
        "    if len(ds['SALT'].shape) < 4:\n",
        "        raise ValueError(\"'SALT' variable expected to have at least 4 dimensions (time, depth, lat, lon).\")\n",
        "\n",
        "    salt_data_3d = ds['SALT'].values[:, 0, :, :]\n",
        "\n",
        "    # Compute the mean salinity for each time step to get a 1D time series\n",
        "    # Use np.nanmean to safely handle NaNs during aggregation\n",
        "    mean_salinity_ts = np.nanmean(salt_data_3d, axis=(1, 2))\n",
        "\n",
        "    # Create a DataFrame for processing\n",
        "    df = pd.DataFrame({'salinity_value': mean_salinity_ts})\n",
        "\n",
        "    # Impute missing values in the 1D mean salinity time series\n",
        "    print(\"Imputing missing values in time series data using median strategy...\")\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    df['salinity_value'] = imputer.fit_transform(df[['salinity_value']])\n",
        "    print(\"Missing values imputed.\")\n",
        "\n",
        "    # Standardize the salinity time series\n",
        "    scaler = StandardScaler()\n",
        "    df['salinity_scaled'] = scaler.fit_transform(df[['salinity_value']])\n",
        "    print(\"Salinity time series standardized.\")\n",
        "\n",
        "    # Create lag features from the *standardized salinity*\n",
        "    # The target `y` will be the current `salinity_scaled`\n",
        "    # The features `X` will be lagged `salinity_scaled` plus time features\n",
        "    print(f\"Creating {n_lags} lag features...\")\n",
        "    for i in range(1, n_lags + 1):\n",
        "        df[f'lag_salinity_{i}'] = df['salinity_scaled'].shift(i)\n",
        "    print(\"Lag features created.\")\n",
        "\n",
        "    # Add time features\n",
        "    # Check if 'TIME' coordinate exists and has datetime accessor\n",
        "    time_feature_added = False\n",
        "    if 'TIME' in ds:\n",
        "        if hasattr(ds['TIME'], 'dt'):\n",
        "            # Ensure that the time coordinate length matches the salinity data\n",
        "            if len(ds['TIME'].values) == len(df):\n",
        "                df['day_of_year'] = ds['TIME'].dt.dayofyear.values\n",
        "                df['month'] = ds['TIME'].dt.month.values\n",
        "                time_feature_added = True\n",
        "            else:\n",
        "                print(\"Warning: Length of 'TIME' coordinate does not match salinity data. Skipping datetime features.\")\n",
        "        else:\n",
        "            # If 'TIME' exists but is not datetime, use its numeric values\n",
        "            if len(ds['TIME'].values) == len(df):\n",
        "                df['time_numeric'] = ds['TIME'].values\n",
        "                time_feature_added = True\n",
        "            else:\n",
        "                print(\"Warning: Length of 'TIME' coordinate does not match salinity data. Skipping numeric time feature.\")\n",
        "\n",
        "    if not time_feature_added:\n",
        "        print(\"Warning: 'TIME' coordinate not found or not suitable for datetime features. Using numeric index as a fallback time feature.\")\n",
        "        df['time_numeric'] = np.arange(len(df)) # Fallback to numeric index\n",
        "\n",
        "    # Drop rows with NaNs created by shifting\n",
        "    initial_rows = len(df)\n",
        "    df = df.dropna()\n",
        "    print(f\"Dropped {initial_rows - len(df)} rows due to NaNs created by lag features or missing time data.\")\n",
        "\n",
        "\n",
        "    # Define features (X) and target (y)\n",
        "    # y is the 'salinity_scaled' column after shifting\n",
        "    # X includes the lag features and time features\n",
        "    y = df['salinity_scaled'].values\n",
        "    X_columns = [col for col in df.columns if col.startswith('lag_salinity_') or col == 'day_of_year' or col == 'month' or col == 'time_numeric']\n",
        "    X = df[X_columns]\n",
        "\n",
        "    print(f\"Features (X) shape: {X.shape}\")\n",
        "    print(f\"Target (y) shape: {y.shape}\")\n",
        "    return X, y, scaler\n",
        "\n",
        "# === 2. GAM Model Creation ===\n",
        "def create_gam_model(X_df):\n",
        "    \"\"\"Create GAM model with proper term construction based on DataFrame columns.\"\"\"\n",
        "    print(\"\\nBuilding GAM model with smoothing and categorical terms...\")\n",
        "    terms = []\n",
        "    # Create terms for each feature based on its position in the DataFrame\n",
        "    for i, col in enumerate(X_df.columns):\n",
        "        if col == 'month':\n",
        "            terms.append(f(i, n_splines=12))  # Categorical feature, 12 categories for months\n",
        "        elif col.startswith('lag_salinity_') or col == 'time_numeric' or col == 'day_of_year':\n",
        "            terms.append(s(i))  # Numerical feature with smoothing spline\n",
        "        else:\n",
        "            # Fallback for any other unexpected columns, treat as smoothing\n",
        "            print(f\"Warning: Unexpected column '{col}'. Treating as smoothing term.\")\n",
        "            terms.append(s(i))\n",
        "\n",
        "    # Initialize LinearGAM with the constructed terms\n",
        "    gam = LinearGAM(terms=TermList(*terms))\n",
        "    print(\"GAM model terms defined.\")\n",
        "    return gam\n",
        "\n",
        "# === 3. Main Execution ===\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    FILE_PATH = \"/content/drive/MyDrive/salinity.nc\"\n",
        "    N_LAGS = 5 # Number of lag features\n",
        "    TEST_SIZE = 0.2 # 20% for testing\n",
        "\n",
        "    # Load and prepare data\n",
        "    try:\n",
        "        X, y, scaler = load_and_prepare_data(FILE_PATH, n_lags=N_LAGS)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        print(\"Exiting program. Please provide the correct dataset path.\")\n",
        "        exit()\n",
        "    except ValueError as e:\n",
        "        print(f\"Data preprocessing failed: {e}\")\n",
        "        print(\"Exiting program.\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during data loading: {e}\")\n",
        "        print(\"Exiting program.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "    if X is not None:\n",
        "        # Train-test split (maintaining temporal order)\n",
        "        print(f\"\\nSplitting data into training and test sets (test_size={TEST_SIZE})...\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=TEST_SIZE, shuffle=False\n",
        "        )\n",
        "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "        # Create GAM model\n",
        "        gam = create_gam_model(X_train) # Pass DataFrame to get column names\n",
        "\n",
        "        # Fit model using gridsearch for automatic parameter tuning\n",
        "        print(\"Fitting GAM model using gridsearch...\")\n",
        "        # pygam's gridsearch expects numpy arrays for X and y\n",
        "        gam.gridsearch(X_train.values, y_train)\n",
        "        print(\"GAM model fitting complete.\")\n",
        "        # FIX: Access the numeric value from the OrderedDict before formatting\n",
        "        if 'pseudo_r2' in gam.statistics_:\n",
        "             pseudo_r2_value = gam.statistics_['pseudo_r2']\n",
        "             # Check if it's a dict/OrderedDict and extract the value if needed\n",
        "             if isinstance(pseudo_r2_value, dict):\n",
        "                 # Assuming the first value is the score if it's an OrderedDict\n",
        "                 pseudo_r2_value = list(pseudo_r2_value.values())[0]\n",
        "             print(f\"Best cross-validation score (Pseudo R-squared): {pseudo_r2_value:.4f}\")\n",
        "        else:\n",
        "            print(\"Pseudo R-squared not found in gam.statistics_ after gridsearch.\")\n",
        "\n",
        "\n",
        "        # Evaluation\n",
        "        print(\"\\nEvaluating GAM model on test set...\")\n",
        "        y_pred = gam.predict(X_test.values) # pygam's predict expects numpy array\n",
        "\n",
        "        # Inverse transform to get original scale values for meaningful metrics and plotting\n",
        "        # y_test and y_pred are currently scaled\n",
        "        y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "        y_pred_orig = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "        # --- Regression Metrics ---\n",
        "        mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test_orig, y_pred_orig) # Calculate MAE\n",
        "        r2 = r2_score(y_test_orig, y_pred_orig)\n",
        "\n",
        "        print(\"\\nðŸ“Š Final Regression Test Metrics:\")\n",
        "        print(f\"âœ… MSE (Mean Squared Error): {mse:.6f}\")\n",
        "        print(f\"âœ… RMSE (Root Mean Squared Error): {rmse:.6f}\")\n",
        "        print(f\"âœ… MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "        print(f\"âœ… RÂ² (R-squared): {r2:.4f}\")\n",
        "\n",
        "        # --- Classification Metrics (for Demonstration Purposes Only) ---\n",
        "        print(\"\\nðŸ”„ Classification Metrics (Derived for Demonstration - See Note Below):\")\n",
        "\n",
        "        # Define a threshold to convert regression output to binary classification.\n",
        "        # Using the median of the true test values as a simple example threshold.\n",
        "        classification_threshold = np.median(y_test_orig)\n",
        "        print(f\" Â (Using classification threshold: {classification_threshold:.4f} based on median of true test values)\")\n",
        "\n",
        "        # Convert true and predicted continuous values to binary classes based on the threshold\n",
        "        y_test_binary = (y_test_orig > classification_threshold).astype(int)\n",
        "        y_pred_binary = (y_pred_orig > classification_threshold).astype(int)\n",
        "\n",
        "        # Calculate classification metrics\n",
        "        accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "        precision = precision_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "        recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)\n",
        "        conf_matrix = confusion_matrix(y_test_binary, y_pred_binary)\n",
        "\n",
        "        print(f\"âœ… Accuracy: Â {accuracy:.4f}\")\n",
        "        print(f\"âœ… Precision: {precision:.4f}\")\n",
        "        print(f\"âœ… Recall: Â  Â {recall:.4f}\")\n",
        "        print(f\"âœ… Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Important Note on Classification Metrics ---\")\n",
        "        print(\"The 'Accuracy', 'Precision', and 'Recall' metrics above are calculated by converting the continuous regression \"\n",
        "              \"outputs into binary classes using an arbitrary threshold (the median of true test values in this case).\")\n",
        "        print(\"This transformation is done *only for demonstration* of these metrics. Your model's primary task is regression, \"\n",
        "              \"and its performance should be primarily judged by MSE, RMSE, MAE, and R-squared.\")\n",
        "        print(\"Choosing a different threshold would likely change these classification metric values.\")\n",
        "\n",
        "        # === 5. Visualization ===\n",
        "        print(\"\\nGenerating plots for visualization...\")\n",
        "\n",
        "        # Plot 1: True vs. Predicted Values Plot (Time Series - Original Scale)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(y_test_orig, label='True Mean Salinity', color='blue', linestyle='-')\n",
        "        plt.plot(y_pred_orig, label='Predicted Mean Salinity', color='red', linestyle='--')\n",
        "        plt.title(\"True vs. Predicted Mean Salinity (Original Scale)\")\n",
        "        plt.xlabel(\"Time Step Index\")\n",
        "        plt.ylabel(\"Mean Salinity\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot 2: Scatter plot of actual vs predicted values with R^2\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(y_test_orig, y_pred_orig, alpha=0.6, color='teal', edgecolor='white', label='Predictions')\n",
        "        max_val = max(y_test_orig.max(), y_pred_orig.max())\n",
        "        min_val = min(y_test_orig.min(), y_pred_orig.min())\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal Prediction')\n",
        "        plt.annotate(f'$R^2$ = {r2:.4f}',\n",
        "                     xy=(0.05, 0.85),\n",
        "                     xycoords='axes fraction',\n",
        "                     fontsize=14,\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "        plt.title('GAM Model Performance: Actual vs Predicted Values', fontsize=14, pad=20)\n",
        "        plt.xlabel('Actual Values', fontsize=12)\n",
        "        plt.ylabel('Predicted Values', fontsize=12)\n",
        "        plt.legend(fontsize=12, loc='upper left')\n",
        "        plt.grid(True, linestyle='--', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot 3: Confusion Matrix\n",
        "        plt.figure(figsize=(7, 6))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "                    yticklabels=['Actual 0', 'Actual 1'])\n",
        "        plt.title('Confusion Matrix for Binary Classification')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot 4: Partial dependence plots for GAM\n",
        "        print(\"\\nGenerating Partial Dependence Plots for GAM terms...\")\n",
        "        try:\n",
        "            titles = X_train.columns # Use training data columns for titles\n",
        "            n_features = len(titles)\n",
        "            rows = int(np.ceil(n_features / 3)) # Max 3 plots per row\n",
        "\n",
        "            plt.figure(figsize=(15, rows * 5))\n",
        "            # Corrected iteration through gam.terms_\n",
        "            for i, term in enumerate(gam.terms_):\n",
        "                if term.isintercept:\n",
        "                    continue\n",
        "\n",
        "                # Access the feature index correctly from the term\n",
        "                feature_index = term.feature\n",
        "                if feature_index < len(titles):\n",
        "                    feature_name = titles[feature_index]\n",
        "                else:\n",
        "                    feature_name = f\"Feature Index {feature_index} (Unknown)\"\n",
        "                    print(f\"Warning: Feature index {feature_index} out of bounds for titles.\")\n",
        "\n",
        "\n",
        "                # Handle different term types for plotting\n",
        "                # Check against the actual type instances\n",
        "                if isinstance(term, s): # Smoothing term\n",
        "                    XX = gam.generate_X_grid(term=i, exclude_na=True)\n",
        "                    pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95, return_std=True)\n",
        "\n",
        "                    plt.subplot(rows, 3, i + 1)\n",
        "                    plt.plot(XX[:, feature_index], pdep, label='Partial Dependence')\n",
        "                    plt.fill_between(XX[:, feature_index], pdep - confi, pdep + confi, alpha=0.2, label='95% Confidence')\n",
        "                    plt.title(f'Partial Dependence of {feature_name}')\n",
        "                    plt.xlabel(feature_name)\n",
        "                    plt.ylabel('Partial Dependence')\n",
        "                    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "                    plt.legend()\n",
        "                elif isinstance(term, f): # Factor (categorical) term\n",
        "                    XX = gam.generate_X_grid(term=i, exclude_na=True)\n",
        "                    pdep = gam.partial_dependence(term=i, X=XX)\n",
        "\n",
        "                    plt.subplot(rows, 3, i + 1)\n",
        "                    # Ensure XX[:, feature_index] is used for x-axis labels if needed,\n",
        "                    # but often for factor plots, simply using range(len(pdep)) and setting xticks is better\n",
        "                    plt.bar(range(len(pdep)), pdep)\n",
        "                    plt.title(f'Partial Dependence of {feature_name}')\n",
        "                    plt.xlabel(feature_name)\n",
        "                    plt.ylabel('Partial Dependence')\n",
        "                    # Attempt to set xticks if feature_name is 'month'\n",
        "                    if feature_name == 'month':\n",
        "                         plt.xticks(range(len(pdep)), XX[:, feature_index].astype(int))\n",
        "                    else:\n",
        "                         plt.xticks(range(len(pdep)), [str(int(x)) for x in XX[:, feature_index]]) # Generic labels for other factors\n",
        "\n",
        "                    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Matplotlib not available - skipping partial dependence plots\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating partial dependence plots: {e}\")\n",
        "\n",
        "\n",
        "        print(\"\\nScript execution complete. Check console for metrics and plots for visualization.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Data loading failed. Cannot proceed with model training.\")"
      ],
      "metadata": {
        "id": "3U-LgdhvPosb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygam\n"
      ],
      "metadata": {
        "id": "LU_tWMQiPrP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}